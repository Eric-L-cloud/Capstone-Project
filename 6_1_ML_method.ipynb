{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_selection import SelectFromModel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipleline_GridSearchCV(gridserachcv, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    gridserachcv.fit(x_train, y_train)\n",
    "    best_params = gridserachcv.best_params_\n",
    "    print(\"Best parameters: \", best_params)\n",
    "    print(\"Best F1 Score:\", gridserachcv.best_score_)\n",
    "    model1_best = gridserachcv.best_estimator_\n",
    "    model1_best = model1_best[1:] # remove the pipeline adasyn\n",
    "    print(\"run score\")\n",
    "    print(model1_best)\n",
    "\n",
    "    feature_df = get_feature_importance(model1_best,x_test.columns)\n",
    "\n",
    "    #SelectFromModel\n",
    "    x_train_selected = x_train\n",
    "    x_test_selected = x_test\n",
    "    if hasattr(model1_best, \"feature_importances_\") or hasattr(model1_best, \"coef_\"):\n",
    "        selector = SelectFromModel(model1_best, threshold='>0', prefit=True)\n",
    "        x_train_selected = selector.transform(x_train)\n",
    "        x_test_selected = selector.transform(x_test)\n",
    "\n",
    "    # x_train2 = x_train.drop(columns= list(feature_df[feature_df['Importance']==0]['Feature']))\n",
    "    # x_test_selected = x_test.drop(columns= list(feature_df[feature_df['Importance']==0]['Feature']))\n",
    "    model1_best.fit(x_train_selected, y_train)\n",
    "\n",
    "    # model1_best.fit(columnsWithoutList(x_train,col_set_feature_engineering), y_train)\n",
    "    rs_train = run_score_1(model1_best, 'train', x_train_selected, y_train,True)\n",
    "    rs_test =  run_score_1(model1_best, 'test', x_test_selected, y_test,True)\n",
    "    rs_train.accuracy_test = rs_test.accuracy\n",
    " \n",
    "    rs_train.accuracy_test = rs_test.accuracy\n",
    "    rs_train.roc_auc_test = rs_test.roc_auc\n",
    "    rs_train.coh_kap_test = rs_test.coh_kap\n",
    "    rs_train.f1_test = rs_test.f1\n",
    "    rs_train.fper_test = rs_test.fper\n",
    "    rs_train.tper_test = rs_test.tper\n",
    "    rs_train.precision_test = rs_test.precision\n",
    "    rs_train.recall_test = rs_test.recall\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return rs_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result_Score:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        self.accuracy = None\n",
    "        self.roc_auc = None\n",
    "        self.coh_kap = None\n",
    "        self.f1 = None\n",
    "        self.fper = None\n",
    "        self.tper = None\n",
    "        self.precision = None\n",
    "        self.recall = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Result_Score:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        self.accuracy = None\n",
    "        self.roc_auc = None\n",
    "        self.coh_kap = None\n",
    "        self.f1 = None\n",
    "        self.fper = None\n",
    "        self.tper = None\n",
    "        self.precision = None\n",
    "        self.recall = None\n",
    "\n",
    "        self.accuracy_test = None\n",
    "        self.roc_auc_test = None\n",
    "        self.coh_kap_test = None\n",
    "        self.f1_test = None\n",
    "        self.fper_test = None\n",
    "        self.tper_test = None\n",
    "        self.precision_test = None\n",
    "        self.recall_test = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_score_1(model, dataType, x, y, verbose=True)->Result_Score:\n",
    "    \n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    roc_auc = roc_auc_score(y, y_pred) \n",
    "    coh_kap = cohen_kappa_score(y, y_pred)\n",
    "   \n",
    "    print(\"---------------------------------------\")\n",
    "    print(dataType)\n",
    "    print(\"  Accuracy = {}\".format(accuracy))\n",
    "    print(\"  F1 Score = {}\".format(f1))\n",
    "    print(\"  ROC Area under Curve = {}\".format(roc_auc))\n",
    "    print(\"  Cohen's Kappa = {}\".format(coh_kap))\n",
    "    print(classification_report(y,y_pred,digits=5))\n",
    "        \n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix - '+dataType)\n",
    "    plt.show()\n",
    "\n",
    "    probs = model.predict_proba(x)  \n",
    "    print(probs)\n",
    "    probs = probs[:, 1]  \n",
    "    fper, tper, thresholds = roc_curve(y, probs) \n",
    "    plot_roc_cur(fper, tper)\n",
    "    \n",
    "    # Calculate precision, recall, and thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probs)\n",
    "    \n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, marker='.', label=model[-1].__class__.__name__)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve\\nF1 Score: {:.3f}'.format(f1))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    rs = Result_Score()\n",
    "    rs.model = model\n",
    "    rs.accuracy = accuracy\n",
    "    rs.roc_auc = roc_auc\n",
    "    rs.coh_kap = coh_kap\n",
    "    rs.f1 = f1\n",
    "    rs.fper = fper\n",
    "    rs.tper = tper\n",
    "    rs.precision = precision\n",
    "    rs.recall = recall\n",
    "\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cur(fper, tper):  \n",
    "    plt.plot(fper, tper, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(modelOrPipeline, x_train_columns):\n",
    "    \n",
    "    if hasattr(modelOrPipeline, 'steps'):\n",
    "        model = modelOrPipeline[-1]\n",
    "    else:\n",
    "        model = modelOrPipeline\n",
    "\n",
    "    importances_df = pd.DataFrame(columns=['Feature','Importance'])\n",
    "      \n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "        # 確保 features 是一維純文字索引\n",
    "        features = pd.Index(x_train_columns).astype(str)\n",
    "\n",
    "        # 建立資料表\n",
    "        importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "        importances_df = importances_df.sort_values(by='Importance', ascending=False)\n",
    "        print(importances_df)\n",
    "        # 畫圖\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importances_df.head(20))\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "    elif  hasattr(model, \"coef_\"):\n",
    "        importances = model.coef_[0]\n",
    "        features = pd.Index(x_train_columns).astype(str)\n",
    "        importances_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "        importances_df = importances_df.reindex(importances_df['Importance'].abs().sort_values(ascending=False).index)\n",
    "        print(importances_df)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importances_df.head(20))\n",
    "        plt.title('Top 20 Feature Importances')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "    return importances_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
